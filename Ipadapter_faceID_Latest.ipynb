{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucianoRodriguez0764/ipadapter_faceid_colab/blob/main/Ipadapter_faceID_Latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing dependencies and download the IP-Adapter model"
      ],
      "metadata": {
        "id": "OB5WejBfJtq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install insightface\n",
        "!pip install onnxruntime\n",
        "!pip install diffusers\n",
        "!pip install git+https://github.com/tencent-ailab/IP-Adapter.git\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "MQzuaaeAThi1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate the image with IP-Adapter"
      ],
      "metadata": {
        "id": "VtB54JeMJT_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O ip-adapter-faceid-plusv2_sd15.bin https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plusv2_sd15.bin"
      ],
      "metadata": {
        "id": "IoeXhrE8QZ_9",
        "outputId": "2a776708-5279-4372-d44a-4dc54cd75290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-25 00:55:38--  https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plusv2_sd15.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.16, 18.239.50.80, 18.239.50.49, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/36/ca/36ca54ff1895000df817b45a5ae71fccd14170b0843d8adc2ba7944d9ac903e9/26d0d86a1d60d6cc811d3b8862178b461e1eeb651e6fe2b72ba17aa95411e313?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ip-adapter-faceid-plusv2_sd15.bin%3B+filename%3D%22ip-adapter-faceid-plusv2_sd15.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1732755338&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMjc1NTMzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM2L2NhLzM2Y2E1NGZmMTg5NTAwMGRmODE3YjQ1YTVhZTcxZmNjZDE0MTcwYjA4NDNkOGFkYzJiYTc5NDRkOWFjOTAzZTkvMjZkMGQ4NmExZDYwZDZjYzgxMWQzYjg4NjIxNzhiNDYxZTFlZWI2NTFlNmZlMmI3MmJhMTdhYTk1NDExZTMxMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=iiArer17l1QpSjhcu-Q%7EvJyuuYSGqrC8oSSK17I7IEvEhDmIG0n8Lj6g60yjJlCVC4OI7UMBlYk098azTXUXmW1mXOuCK17XditrpTB3V5rCc2lvHT9DuWJLWlJ-Cxfia9cyP0Cr8aPiGj%7E-e39bXw3duwRPTprLnlxaKVIqzKUhYVp7kyx1l5yccjmj7g9WjESo6s6r5li7QNxSREw6vobc4gN-YQ5CJj1KSuOcB1f-wn6TJ8mHRWHBYtRZpD7bHIjpiktELgboCIRtxjPlKjh7Jqsiw8GIp6-mWIs7GA4fFpAzzRoW5QnK2sGwTeR8gLxtc7LqsUV%7E7oVIPEnBdA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-11-25 00:55:38--  https://cdn-lfs-us-1.hf.co/repos/36/ca/36ca54ff1895000df817b45a5ae71fccd14170b0843d8adc2ba7944d9ac903e9/26d0d86a1d60d6cc811d3b8862178b461e1eeb651e6fe2b72ba17aa95411e313?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ip-adapter-faceid-plusv2_sd15.bin%3B+filename%3D%22ip-adapter-faceid-plusv2_sd15.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1732755338&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMjc1NTMzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM2L2NhLzM2Y2E1NGZmMTg5NTAwMGRmODE3YjQ1YTVhZTcxZmNjZDE0MTcwYjA4NDNkOGFkYzJiYTc5NDRkOWFjOTAzZTkvMjZkMGQ4NmExZDYwZDZjYzgxMWQzYjg4NjIxNzhiNDYxZTFlZWI2NTFlNmZlMmI3MmJhMTdhYTk1NDExZTMxMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=iiArer17l1QpSjhcu-Q%7EvJyuuYSGqrC8oSSK17I7IEvEhDmIG0n8Lj6g60yjJlCVC4OI7UMBlYk098azTXUXmW1mXOuCK17XditrpTB3V5rCc2lvHT9DuWJLWlJ-Cxfia9cyP0Cr8aPiGj%7E-e39bXw3duwRPTprLnlxaKVIqzKUhYVp7kyx1l5yccjmj7g9WjESo6s6r5li7QNxSREw6vobc4gN-YQ5CJj1KSuOcB1f-wn6TJ8mHRWHBYtRZpD7bHIjpiktELgboCIRtxjPlKjh7Jqsiw8GIp6-mWIs7GA4fFpAzzRoW5QnK2sGwTeR8gLxtc7LqsUV%7E7oVIPEnBdA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.65.39.32, 18.65.39.117, 18.65.39.65, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.65.39.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 156558509 (149M) [application/octet-stream]\n",
            "Saving to: ‘ip-adapter-faceid-plusv2_sd15.bin’\n",
            "\n",
            "ip-adapter-faceid-p 100%[===================>] 149.31M  40.9MB/s    in 3.6s    \n",
            "\n",
            "2024-11-25 00:55:42 (40.9 MB/s) - ‘ip-adapter-faceid-plusv2_sd15.bin’ saved [156558509/156558509]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlus\n",
        "\n",
        "v2 = True\n",
        "base_model_path = \"SG161222/Realistic_Vision_V4.0_noVAE\"\n",
        "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
        "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
        "ip_ckpt = \"ip-adapter-faceid-plus_sd15.bin\" if not v2 else \"ip-adapter-faceid-plusv2_sd15.bin\"\n",
        "device = \"cuda\"\n",
        "\n",
        "noise_scheduler = DDIMScheduler(\n",
        "    num_train_timesteps=1000,\n",
        "    beta_start=0.00085,\n",
        "    beta_end=0.012,\n",
        "    beta_schedule=\"scaled_linear\",\n",
        "    clip_sample=False,\n",
        "    set_alpha_to_one=False,\n",
        "    steps_offset=1,\n",
        ")\n",
        "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    base_model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    scheduler=noise_scheduler,\n",
        "    vae=vae,\n",
        "    feature_extractor=None,\n",
        "    safety_checker=None\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "-vXAlOnhQjDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from insightface.app import FaceAnalysis\n",
        "from insightface.utils import face_align\n",
        "import torch\n",
        "import glob, os\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "app.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "image_path = \"images/embeds/1\"\n",
        "imgs = glob.glob(os.path.join(image_path, '*.*'))\n",
        "#imgs += glob.glob(os.path.join('images/face_2', '*.*'))\n",
        "\n",
        "# Collect all embeddings\n",
        "embeddings = []\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "    print(imgs[i])\n",
        "    image = cv2.imread(imgs[i])\n",
        "    faces = app.get(image)\n",
        "\n",
        "    # Check if a face is detected\n",
        "    if faces:\n",
        "        faceid_embed = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
        "        embeddings.append(faceid_embed)\n",
        "    else:\n",
        "        print(\"face not detected\")\n",
        "\n",
        "# Calculate the average embedding for the face ID\n",
        "\n",
        "weights = []\n",
        "weights = [elem/sum(weights) for elem in weights]\n",
        "#print(weights)\n",
        "\n",
        "if embeddings:\n",
        "\n",
        "    ### weights\n",
        "    total = torch.zeros_like(embeddings[0])\n",
        "    for i in range(len(embeddings)):\n",
        "        if len(weights)==len(embeddings):\n",
        "            total.add_(embeddings[i]*weights[i])\n",
        "        else:\n",
        "            total.add_(embeddings[i]*(1/len(embeddings)))\n",
        "    faceid_embeds = total / len(embeddings[i])\n",
        "\n",
        "    ###  mean weights\n",
        "    faceid_embeds_mean = torch.mean(torch.stack(embeddings), dim=0)\n",
        "\n",
        "    print(\"Collected face ID embedding\")\n",
        "else:\n",
        "    print(\"No faces detected in the images provided.\")\n",
        "\n",
        "\n",
        "\n",
        "# face\n",
        "image_path_face = \"images/face\"\n",
        "imgs_2 = glob.glob(os.path.join(image_path_face, '*.*'))\n",
        "image1 = cv2.imread(imgs_2[0])\n",
        "faces1 = app.get(image1)\n",
        "if faces1:\n",
        "  print(\"face image detected\")\n",
        "else:\n",
        "  print(\"face image not detected\")\n",
        "\n",
        "face_image = face_align.norm_crop(image1, landmark=faces1[0].kps, image_size=336)"
      ],
      "metadata": {
        "id": "Q5cc28eQQrva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# face\n",
        "image_path_face = \"images/face\"\n",
        "imgs_2 = glob.glob(os.path.join(image_path_face, '*.*'))\n",
        "image1 = cv2.imread(imgs_2[3])\n",
        "faces1 = app.get(image1)\n",
        "if faces1:\n",
        "  print(\"face image detected\")\n",
        "else:\n",
        "  print(\"face image not detected\")\n",
        "\n",
        "face_image4 = face_align.norm_crop(image1, landmark=faces1[0].kps, image_size=336)\n",
        "\n",
        "image1 = cv2.imread(imgs_2[2])\n",
        "faces1 = app.get(image1)\n",
        "if faces1:\n",
        "  print(\"face image detected\")\n",
        "else:\n",
        "  print(\"face image not detected\")\n",
        "\n",
        "face_image3 = face_align.norm_crop(image1, landmark=faces1[0].kps, image_size=336)\n",
        "\n",
        "image1 = cv2.imread(imgs_2[1])\n",
        "faces1 = app.get(image1)\n",
        "if faces1:\n",
        "  print(\"face image detected\")\n",
        "else:\n",
        "  print(\"face image not detected\")\n",
        "\n",
        "face_image2 = face_align.norm_crop(image1, landmark=faces1[0].kps, image_size=336)\n",
        "\n",
        "image1 = cv2.imread(imgs_2[0])\n",
        "faces1 = app.get(image1)\n",
        "if faces1:\n",
        "  print(\"face image detected\")\n",
        "else:\n",
        "  print(\"face image not detected\")\n",
        "\n",
        "face_image1 = face_align.norm_crop(image1, landmark=faces1[0].kps, image_size=336)"
      ],
      "metadata": {
        "id": "2cmjR6LO14DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from ipywidgets import HBox, Image\n",
        "import random\n",
        "\n",
        "def display_images(images, format=\"png\"):\n",
        "  if format==\"png\" or True:\n",
        "    image_widgets = [Image(value=img._repr_png_()) for img in images]\n",
        "  # Display the images in a row\n",
        "  display(HBox(image_widgets))\n",
        "\n",
        "prompt = \"\"\n",
        "prompt += \"20 years old girl, skinny and bonny, full body, kneeling on bed, serious face, mouth closed, wearing a skirt\"\n",
        "extra_prompt = \"\"\n",
        "\n",
        "negative_prompt = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime), text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, nfsw, extra legs, fused fingers, too many fingers, long neck\"\n",
        "negative_prompt += \"\"\n",
        "\n",
        "width = 512\n",
        "height = 512\n",
        "num_inference_steps = 50\n",
        "guidance_scale = 2\n",
        "num_samples = 3\n",
        "random_seed = 4574673546\n",
        "use_hardcoded_seed = False\n",
        "v2=True\n",
        "s_scale = 0.1\n",
        "\n",
        "x_plot_generation = False\n",
        "y_plot_generation = False\n",
        "x_plot_parameter = 's_scale'\n",
        "x_plot_values = [0.1,0.3,0.6,1.0,1.5,2.0]\n",
        "y_plot_parameter = 'num_inference_steps'\n",
        "y_plot_values = [30,40,50]\n",
        "xy_different_seeds = False\n",
        "face_image = face_image\n",
        "\n",
        "\n",
        "ip_model = IPAdapterFaceIDPlus(pipe, image_encoder_path, ip_ckpt, device)\n",
        "\n",
        "if not use_hardcoded_seed:\n",
        "  random_seed = random.randint(1, 9999999)\n",
        "\n",
        "if x_plot_generation:\n",
        "  num_samples=1\n",
        "  if y_plot_generation:\n",
        "    images_array = []\n",
        "    for h in range(len(y_plot_values)):\n",
        "      images = []\n",
        "      if y_plot_parameter in globals():\n",
        "        globals()[y_plot_parameter] = y_plot_values[h]\n",
        "        if y_plot_parameter != \"face_image\" and y_plot_parameter != \"faceid_embeds\":\n",
        "          print(y_plot_parameter,\"set to:\",y_plot_values[h])\n",
        "      for i in range(len(x_plot_values)):\n",
        "        if x_plot_parameter in globals():\n",
        "          globals()[x_plot_parameter] = x_plot_values[i]\n",
        "          if x_plot_parameter != \"face_image\" and x_plot_parameter != \"faceid_embeds\":\n",
        "            print(x_plot_parameter,\"set to:\",x_plot_values[i])\n",
        "        if xy_different_seeds and i!=0:\n",
        "          random_seed = random.randint(1, 9999999)\n",
        "        print(\"Seed: \", random_seed)\n",
        "        generated_image = ip_model.generate(\n",
        "      prompt=prompt+\", \"+extra_prompt, negative_prompt=negative_prompt, face_image=face_image, faceid_embeds=faceid_embeds_mean, num_samples=num_samples, width=width, height=height, num_inference_steps=num_inference_steps, seed=random_seed,\n",
        "      guidance_scale=guidance_scale, shortcut=True, s_scale=s_scale,\n",
        "  )[0]\n",
        "        images.append(generated_image)\n",
        "      images_array.append(images)\n",
        "    for img in images_array:\n",
        "      display_images(img)\n",
        "  elif x_plot_generation:\n",
        "    images = []\n",
        "    for i in range(len(x_plot_values)):\n",
        "      if x_plot_parameter in globals():\n",
        "        globals()[x_plot_parameter] = x_plot_values[i]\n",
        "        if x_plot_parameter != \"face_image\" and x_plot_parameter != \"faceid_embeds\":\n",
        "          print(x_plot_parameter,\"set to:\",x_plot_values[i])\n",
        "      if xy_different_seeds and i!=0:\n",
        "        random_seed = random.randint(1, 9999999)\n",
        "      print(\"Seed: \", random_seed)\n",
        "      images.extend(ip_model.generate(\n",
        "      prompt=prompt+\", \"+extra_prompt, negative_prompt=negative_prompt, face_image=face_image, faceid_embeds=faceid_embeds_mean, num_samples=num_samples, width=width, height=height, num_inference_steps=num_inference_steps, seed=random_seed,\n",
        "      guidance_scale=guidance_scale, shortcut=True, s_scale=s_scale,\n",
        "  ))\n",
        "    display_images(images)\n",
        "\n",
        "\n",
        "else:\n",
        "\n",
        "  print(\"Seed: \", random_seed)\n",
        "  images = ip_model.generate(\n",
        "      prompt=prompt+\", \"+extra_prompt, negative_prompt=negative_prompt, face_image=face_image, faceid_embeds=faceid_embeds_mean, num_samples=num_samples, width=width, height=height, num_inference_steps=num_inference_steps, seed=random_seed,\n",
        "      guidance_scale=guidance_scale, shortcut=True, s_scale=s_scale,\n",
        "  )\n",
        "\n",
        "  display_images(images)"
      ],
      "metadata": {
        "id": "VKYmt7DxQk36"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}